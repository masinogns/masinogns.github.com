---
layout: post
title: 내 모델 설계 - 데이터
categories: MachineLearning
tag: ml
---

## 데이터 다루기.
* 1주일 데이터를 하나로 묶는다.
* 그래프를 겹치게 한다.
* 1/n을 취한다.
* 1주일

생각해 볼 문제
윤달의 처리는 대세에 큰 영향을 미칠 것 같지는 않다고 생각한다.
그래서 굳이 윤달의 처지를 할 필요는 없을 것 같긴함.
생각해봐야 할 문제.

비교 rmse는 표준 공식이 있기 때문에 있는 그대로 저장해두는게 좋고
내가 볼 때만 %가 유용하다.
즉, 다른 케이스와의 비교할 때는 원래의 rmse가 필요하다(%가 없는).

## 데이터 테스트 케이스(데이터 전처리 과정)
* 하루 24시간을 시퀀스로 넣는다. 아웃풋은 한 시간의 생산량을 예측한다.(seq_length=24, input_dim = 9)
* 하루 24시간의 데이터들을 평균을 낸 후에 시퀀스로 한 주(seq_lenght=7, input_dim = 7)를 넣는다. 아웃풋은 하루의 생산량을 예측한다.
* 한 주의 데이터들을 평균을 낸 후에 시퀀스로 한 달(seq_length=4)을 넣는다. 아웃풋은 한 주의 생산량을 예측한다.
* 한 달의 데이터들을 평균을 낸 후에 시퀀스로 6개월(seq_length=6)을 넣는다. 아웃풋은 한 달의 생산량을 예측한다.

### 여기서 가장 좋은 RMSE를 하면 된다.

## 학습률이랑 loop 찾기
adam이라는 알고리즘에 학습률을 넣기 때문에 하나를 찾아두면 그냥 그 학습률을 넣으면 될 것 같다.
레이어를 조작하면서 더욱 좋은 rmse를 찾도록 한다.


## 테스트 과정은?
* 가장 낮은 rmse를 갖는 학습률을 찾는다.
* 여러 데이터 케이스에 같은 학습률을 가지고 테스팅을 하고 rmse를 비교한다.
* 그리고 가장 낮은 rmse를 가지고 있는 데이터 케이스를 가지고 레이어를 조작한다.
* 레이어 쌓거나 dropout도 하면서 가장 낮은 rmse를 찾으면 끝.
* rmse는 멈추는거다. 계속 같은 것으로 쭉가고 안떨어지면 거기서 끝.
* rmse가 멈추는 부분에서 이 데이터가 좋은 데이터라는 것을 알 수 있는 반증이다.

## 내꺼랑 현우꺼 비교 생각
rnn은 예측하면서 어떤 행동을 해야할지 예측해서 말해주면 rnn이다. 예측 문제 !! Regression 문제<br>
cnn은 어떤 패턴을 인식해서 무엇인지 말해주는 것. 다음에 뭘 할지 나는 몰라. 인식 문제 !! Classification 문제

#### 시간 정보
1시간 안에 10만장의 프레임이 있다고 생각할 수 있다.
그런데 얼굴을 인식하는 건 cnn으로 10만 장의 프레임을 랜덤하게 섞게되도 얼굴만 인식하는 거고
지금 싸우는 것인지 아닌지 예측하면서 하는건 rnn이기때문에 프레임 간의 시간 정보가 필요하다.

## 모델의 성능을 더 좋게 만들기 위한 몇가지 기법이 있습니다:

* 학습 속도 감소를 스케줄링 하기,Exponential learning rate
* LSTM 레이어 간에 드롭아웃 적용.


## 기타 송현 생각해 볼 문제

Sketch rnn.
Distortion 이미지를 일그러뜨리고 노이즈를 넣게하고 데이터 뻥튀기하는거.
Google cloud platform.
Google inception v3 —> retrain.py
Image learning.

## 실험 결과

평균을 취한 값을 하니까 트레이닝 시킬 에러가 너무 많이 튀고 rmse도 너무 높게 나온다.
그래서 loop를 원래는 1000번만 하는데 10000번으로 10배 상향시켰는데도 안됨.
1000번보다는 loss가 많이 떨어지긴 하지만 rmse는 그대로.
그냥 데이터가 2만개에서 /24를 해서 800개로 줄어든게 문제인가 생각함.
아니면 그냥 시간 데이터에서는 패턴이 들쭉날쭉해서 rmse가 높게 나온건데
하루 데이터에서는 이제 패턴이 줄어들어 패턴을 찾지못해 학습이 안되고 결국 예측도 안되는 걸로 예측됨.

GAN 도 하나의 방법이 될 수 있을 거라 생각이 되어짐
GAN 이 아니라 GRU

처음에 last 데이터로 2년의 데이터 중에서 1달치를 넣고 학습을 시키고 했는데 loss가 적고 rmse가 적게 나왔는데 2년의 데이터를 다 넣고 학습을 시키면 loss가 크고 rmse는 비슷하게 나오더라.

왜 이런걸까?

일단 1달에서의 가장 좋은 rmse가 나오는 learning rate가 2년치를 학습할 때 learning rate 에서 나오는 rmse가 다르다.

## 문제 발생

어떤 것 인가

문제 발생 : LSTM 레이어를 5개 이상 쌓기 시작하면 predict가 동작을 하지 못함
즉 일직선으로 쭉 하나의 값으로 예측만 해 나간다.

일직선으로 나오는 이유는 learning rate를 0.1로 비교적 크게 잡아서 그런 것 같다.


에러가 높게 나오고 rmse는 비슷하게 나온다.
rmse는 비슷하게 나오는데 에러는 서로 크게 차이가 나는 이유는?

dropout은 predict가 일직선으로 나오는 것과는 상관이 없다.

dropout은 RNN에서는 성능을 저하시킨다는 말이 있다.
Recurrent neural network regularization

Motivation: Dropout을 RNN에 그냥 적용하면 성능이 좋지 않다
Idea: Dropout을 모든 connection에 적용하는 대신 non recurrent connection에만 dropout을 적용하자

Machine Learning 모델을 만들다보면 Hyperparameter 라는 녀석을 다뤄야 하는 일이 종종 발생한다. 예를 들면 Random forest의 forest 개수라거나, Neural network의 layer 개수, learning rate, momentum 값 등등.. 문제는 이런 hyperparameter들을 어떻게 설정하느냐에 따라 그 결과가 크게 바뀌기 때문에 소위 말하는 ‘튜닝’에 시간을 매우 많이 쏟아야한다는 점이다.http://sanghyukchun.github.io/99/


레이어를 늘린다고 성능이 좋게 나오는 것도 아니였다. 특정 레이어 갯수 이상에서는 아예 예측이 안먹는 상황이 발생했다. Learning rate를 줄이니까 되더라.
Learning rate를 무작정 0.1로만 한다고 좋은 것도 아니였다.
데이터 마다 레이어 마다 맞는 learning rate가 존재하는 듯 하다.

rnn은 dropout을 하면 되려 성능이 떨어진다고 한다고 했는데 이게 또 그러지 않은 것 같다. 구글에서 tensorflow의 dropout을 rnn에 맞게 구현해 놓았을 수 있는 것 같다.

에러가 또 뛰기 시작하면
Learning rate가 큰것이다.
줄여서 학습시켜보자.
